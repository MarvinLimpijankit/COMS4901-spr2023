{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_prep.ipynb\n",
    "\n",
    "Name: Marvin Limpijankit  \n",
    "Email: ml4431@columbia.edu  \n",
    "  \n",
    "This notebook covers the entire process of scraping the data and preprocessing it before prior to analysis. It is separated into modules so that certain sections can be ran with slight changes to the parameters for ease of reproducibility and further extension. In my analysis, I collect 100 images from the search term \"Ukraine\" from 6 websites (3 Chinese, 3 Western), which are in the */data* directory of this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "sys.path.insert(0, '/Users/marvinlimpijankit/Documents/GitHub/COMS4901-spr2023/lib')\n",
    "\n",
    "from image_scraping import google_image_scrape\n",
    "from image_normalization import find_smallest_img_size, normalize_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**google_image_scrape** is a method that takes (search_term, website_domain, num_of_images, output_path) as inputs and saves the top *num_of_images* images with the *search_term* restricted to a certain *website_domain* at *output_path*. The images are saved into the path: \"*output_path/[website_domain].[search_term]*\"\n",
    "  \n",
    "An example call is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_image_scrape('Ukraine', 'nytimes.com', 10, '../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The result of running the above can be found in the 'data' folder. For ease of readability, the results from running the above for the six websites in this analysis, manually filtering for duplicates, and filtering only .jpg images are saved in data in two folders titled, 'China' and 'Western' which contains sub-folders corresponding to each of the news media sources containing 100 images each.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the raw data is ready, we parse through the images, finding the smallest dimension in both the x and y direction in order to normalize them into smaller, square images. This step is done for color analysis, as we want each image to contain the same number of pixels and it is easier to work with same-size, smaller images. \n",
    "\n",
    "*it is possible to run the analysis on the native sizes but all methods would need to additionally account for size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum width across all images is: 156 pixels\n",
      "The minimum height across all images is: 166 pixels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(156, 166)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store a list of website names, along with their region\n",
    "websites = [('CNN', 'US'), ('NBC', 'US'), ('NYT', 'US'), ('China_Daily', 'China'), \n",
    "            ('People\\'s_Daily', 'China'), ('Xinhua_News_Agency', 'China')]\n",
    "\n",
    "# find the smallest x, y dimensions across all images\n",
    "find_smallest_img_size('../data', websites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we'll choose to reduce the image size to a square image of size (128, 128) and save this into the 'output' folder, where we keep intermediate data files produced in this analysis. The *normalize_imgs* method also renames each image to img_0.jpg - img_100.jpg for each website, making it easier to locate them for later qualitative analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have been normalized to size (128, 128) and saved at path \"../output/normalized(128, 128)\".\n"
     ]
    }
   ],
   "source": [
    "# normalize all the images in a 128x128 square format\n",
    "normalize_imgs('../output', '../data', websites, (128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset and normalized images data prep is now concluded and we can move forward with different parts of the analysis which are covered in their respective .ipynb files in the doc directory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
